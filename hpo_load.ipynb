{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29c1e78",
   "metadata": {},
   "source": [
    "# load necessary stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6f9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download link: https://github.com/obophenotype/human-phenotype-ontology/releases\n",
    "# phenotype.hpoa : https://github.com/obophenotype/human-phenotype-ontology/blob/fd050e5aea1d01bd3e6b4e524acb7289e1f3e266/docs/annotations/genes_to_phenotype.md\n",
    "# phenotype_to_genes.txt : https://github.com/obophenotype/human-phenotype-ontology/blob/fd050e5aea1d01bd3e6b4e524acb7289e1f3e266/docs/annotations/phenotype_to_genes.md\n",
    "# genes_to_phenotype.txt : https://github.com/obophenotype/human-phenotype-ontology/blob/fd050e5aea1d01bd3e6b4e524acb7289e1f3e266/docs/annotations/genes_to_phenotype.md\n",
    "\n",
    "import obonet\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Configs:\n",
    "    hpo_files_path = \"docs/\"\n",
    "    snomed_file_path = \"snomed/\"\n",
    "    embedding_model_hf = \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n",
    "    output_embedding_folder = \"embeddings_sapbert_and_info/\"\n",
    "    output_mapping_folder = \"output_mapping/\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddc4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_name):\n",
    "    return pl.read_csv(file_name, separator=\"\\t\", comment_prefix = \"#\")\n",
    "def read_parquet(file_name, cols):\n",
    "    return pl.read_parquet(file_name, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4203388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology\n",
    "graph = obonet.read_obo(f\"{Configs.hpo_files_path}hp.obo\")\n",
    "\n",
    "# Load phenotype-disease mappings\n",
    "df_hpoa = read_txt(f\"{Configs.hpo_files_path}phenotype.hpoa\")\n",
    "df_p2g = read_txt(f\"{Configs.hpo_files_path}phenotype_to_genes.txt\")\n",
    "df_g2p = read_txt(f\"{Configs.hpo_files_path}genes_to_phenotype.txt\")\n",
    "\n",
    "# get release data (for label and synonym)\n",
    "df_snomed_release = read_parquet(f\"{Configs.snomed_file_path}released_version.parquet\", ['id',\"term\"])\n",
    "\n",
    "# get our snomed data (for semantic tag / top category), and limit to only findings\n",
    "df_concept_snomed = read_parquet(f\"{Configs.snomed_file_path}concept_snomed_hug.parquet\", ['id',\"label\",\"concept_type\",\"top_category\"])\n",
    "df_concept_snomed = df_concept_snomed.filter((pl.col(\"concept_type\") ==\"SCT_PRE\") & (pl.col(\"top_category\") == \"finding\"))\n",
    "fiding_snomed = df_concept_snomed['id'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c65b2",
   "metadata": {},
   "source": [
    "# hpo graph get all nodes and useful info into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c864be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare rows for the node dataframe\n",
    "node_rows = []\n",
    "\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    node_rows.append({\n",
    "        \"hpo_id\": node_id,\n",
    "        \"label\" : data.get(\"name\", None),\n",
    "        \"def\": data.get(\"def\", None),\n",
    "        \"comment\": data.get(\"comment\", None),\n",
    "        \"synonyms\": data.get(\"synonym\", []),\n",
    "        \"xrefs\": data.get(\"xref\", []),\n",
    "        \"is_a\": data.get(\"is_a\", []),\n",
    "        \"is_obsolete\": data.get(\"is_obsolete\", \"false\")\n",
    "    })\n",
    "df_concept_hpo = pl.DataFrame(node_rows).filter(pl.col(\"is_obsolete\") == \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b218b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hpo mappable list\n",
    "concept_hpo_mappable = df_concept_hpo.explode(\"xrefs\").filter(pl.col(\"xrefs\").str.contains(\"SNOMED\"))[\"hpo_id\"].unique()\n",
    "\n",
    "# get hpo non mappable dataframe\n",
    "df_concept_hpo_not_mappable = df_concept_hpo.filter(~pl.col(\"hpo_id\").is_in(concept_hpo_mappable))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10289fe",
   "metadata": {},
   "source": [
    "# map the HPO to SNOMED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732bcdc",
   "metadata": {},
   "source": [
    "## Mappable part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9c6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept_hpo_mappable = df_concept_hpo.filter(pl.col(\"hpo_id\").is_in(concept_hpo_mappable))\n",
    "df_concept_hpo_mappable = (df_concept_hpo_mappable\n",
    "                           .select(\"hpo_id\",\"label\", \"xrefs\")\n",
    "                           .explode(\"xrefs\")\n",
    "                           .filter(pl.col(\"xrefs\").str.contains(\"SNOMEDCT\"))\n",
    "                           .with_columns(pl.col(\"xrefs\").str.split(\":\"))\n",
    "                           .explode(\"xrefs\")\n",
    "                           .filter(~pl.col(\"xrefs\").str.contains(\"SNOMEDCT\")))\n",
    "df_snomed_hpo_mappable = df_concept_hpo_mappable.join(df_snomed_release, left_on=\"xrefs\", right_on=\"id\").rename({\"label\": \"hpo_label\", \"xrefs\": \"snomed_id\", \"term\": \"snomed_label\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2ff7d",
   "metadata": {},
   "source": [
    "## Non-mappable part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d1b49",
   "metadata": {},
   "source": [
    "### finetuned lora on SapBERT embedding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975a017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def top_k_array_by_batch(query_matrix, candidate_matrix, batch_size=100, k=2, device=\"cuda\"):\n",
    "\n",
    "    # Convert to torch tensors if they are numpy arrays\n",
    "    if not isinstance(query_matrix, torch.Tensor):\n",
    "        query_matrix = torch.tensor(query_matrix, dtype=torch.float32, device=device)\n",
    "\n",
    "    if not isinstance(candidate_matrix, torch.Tensor):\n",
    "        candidate_matrix = torch.tensor(candidate_matrix, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Normalize vectors\n",
    "    query_matrix = nn.functional.normalize(query_matrix, p=2, dim=1)\n",
    "    candidate_matrix = nn.functional.normalize(candidate_matrix, p=2, dim=1)\n",
    "\n",
    "    candidate_matrix_T = candidate_matrix.T\n",
    "\n",
    "    num_concepts = query_matrix.shape[0]\n",
    "    num_batches = (num_concepts + batch_size - 1) // batch_size\n",
    "\n",
    "    all_top_scores = []\n",
    "    all_top_indices = []\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_concepts)\n",
    "\n",
    "        print(f\"Processing batch {batch_idx + 1}/{num_batches} ({start_idx}-{end_idx})\")\n",
    "\n",
    "        query_batch = query_matrix[start_idx:end_idx]\n",
    "\n",
    "        # Compute cosine similarity scores\n",
    "        scores = torch.matmul(query_batch, candidate_matrix_T)\n",
    "\n",
    "        # Directly get top-k scores and indices\n",
    "        top_scores, top_indices = torch.topk(scores, k=k, dim=1)\n",
    "\n",
    "        all_top_scores.append(top_scores.cpu())\n",
    "        all_top_indices.append(top_indices.cpu())\n",
    "\n",
    "        # Free memory explicitly if using GPU\n",
    "        del scores, top_scores, top_indices\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    all_top_scores = torch.cat(all_top_scores, dim=0)\n",
    "    all_top_indices = torch.cat(all_top_indices, dim=0)\n",
    "\n",
    "    return all_top_scores, all_top_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfd21b",
   "metadata": {},
   "source": [
    "### embed them, save all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a728460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cambridgeltl/SapBERT-from-PubMedBERT-fulltext. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Download from the 🤗 Hub\n",
    "base = Configs.embedding_model_hf\n",
    "# mine = \"yyzheng00/sapbert_lora_triplet_rank16_merged\"\n",
    "\n",
    "model = SentenceTransformer(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", device=Configs.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce1125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snomed_release_finding = df_snomed_release.filter(pl.col(\"id\").is_in(fiding_snomed))\n",
    "\n",
    "df_snomed_release_finding.insert_column(0,pl.Series(np.arange(len(df_snomed_release_finding))).alias(\"index_snomed\"))\n",
    "labels_snomed = df_snomed_release_finding['term'].to_list()\n",
    "index2snomedid = dict(zip(df_snomed_release_finding[\"index_snomed\"], df_snomed_release_finding[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8059229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept_hpo_not_mappable.insert_column(0,pl.Series(np.arange(len(df_concept_hpo_not_mappable))).alias(\"index_hpo\"))\n",
    "labels_hpo = df_concept_hpo_not_mappable['label'].to_list()\n",
    "index2hpoid = dict(zip(df_concept_hpo_not_mappable[\"index_hpo\"], df_concept_hpo_not_mappable[\"hpo_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c5e636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed70443fcf54609a8876b037226dd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349c8f646ebf411a85b6bff70ecb4198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_snomed = model.encode(labels_snomed, show_progress_bar=True)\n",
    "embedding_hpo = model.encode(labels_hpo,  show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffcae6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "df_snomed_release_finding.write_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_snomed_release_finding.parquet\")\n",
    ")\n",
    "\n",
    "df_concept_hpo_not_mappable.write_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_concept_hpo_not_mappable.parquet\")\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_snomed.npy\"),\n",
    "    embedding_snomed\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_hpo.npy\"),\n",
    "    embedding_hpo\n",
    ")\n",
    "\n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2hpoid.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(index2hpoid, f)\n",
    "\n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2snomedid.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(index2snomedid, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b5f80",
   "metadata": {},
   "source": [
    "### load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "df_snomed_release_finding = pl.read_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_snomed_release_finding.parquet\")\n",
    ")\n",
    "\n",
    "df_concept_hpo_not_mappable = pl.read_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_concept_hpo_not_mappable.parquet\")\n",
    ")\n",
    "\n",
    "embedding_snomed = np.load(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_snomed.npy\")\n",
    ")\n",
    "\n",
    "embedding_hpo = np.load(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_hpo.npy\")\n",
    ")\n",
    "\n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2hpoid.pkl\"), \"rb\") as f:\n",
    "    index2hpoid = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2snomedid.pkl\"), \"rb\") as f:\n",
    "    index2snomedid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09bafb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/16 (0-1000)\n",
      "Processing batch 2/16 (1000-2000)\n",
      "Processing batch 3/16 (2000-3000)\n",
      "Processing batch 4/16 (3000-4000)\n",
      "Processing batch 5/16 (4000-5000)\n",
      "Processing batch 6/16 (5000-6000)\n",
      "Processing batch 7/16 (6000-7000)\n",
      "Processing batch 8/16 (7000-8000)\n",
      "Processing batch 9/16 (8000-9000)\n",
      "Processing batch 10/16 (9000-10000)\n",
      "Processing batch 11/16 (10000-11000)\n",
      "Processing batch 12/16 (11000-12000)\n",
      "Processing batch 13/16 (12000-13000)\n",
      "Processing batch 14/16 (13000-14000)\n",
      "Processing batch 15/16 (14000-15000)\n",
      "Processing batch 16/16 (15000-15719)\n"
     ]
    }
   ],
   "source": [
    "# for loop over hpo to find the most slimilar snomed\n",
    "all_top_scores, all_top_indices = top_k_array_by_batch(embedding_hpo, embedding_snomed, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f62d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_snomed_id = []\n",
    "hpo_id_to_map = []\n",
    "mapped_snomed_label = []\n",
    "hpo_label_to_map = []\n",
    "scores_mapping = []\n",
    "\n",
    "for hpo_index in np.arange(len(all_top_scores)):\n",
    "    scores = all_top_scores[hpo_index,:]\n",
    "    mapped_snomed_index = all_top_indices[hpo_index,:]\n",
    "    score = scores[0]\n",
    "    \n",
    "    hpo_ind = index2hpoid.get(hpo_index)\n",
    "    snomed_ind = index2snomedid.get(mapped_snomed_index[0].item())\n",
    "    label_hpo = df_concept_hpo_not_mappable.filter(pl.col(\"hpo_id\") == hpo_ind)['label']\n",
    "    label_snomed = df_snomed_release_finding.filter(pl.col(\"id\") == snomed_ind)['term']\n",
    "    \n",
    "    mapped_snomed_id.append(snomed_ind)\n",
    "    hpo_id_to_map.append(hpo_ind)\n",
    "    mapped_snomed_label.append(label_snomed)\n",
    "    hpo_label_to_map.append(label_hpo)\n",
    "    scores_mapping.append(score)\n",
    "\n",
    "df_snomed_hpo_home_mapped = pl.DataFrame({\n",
    "    \"hpo_id\": hpo_id_to_map,\n",
    "    \"hpo_label\": hpo_label_to_map,\n",
    "    \"snomed_id\": mapped_snomed_id,\n",
    "    \"snomed_label\": mapped_snomed_label,\n",
    "    \"score_top_1\": scores_mapping,\n",
    "    \"mapping_type\": \"sapbert\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85d71bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "sys:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "df_hop_snomed_mapping = pl.concat([df_snomed_hpo_home_mapped,\n",
    "                                df_snomed_hpo_mappable.with_columns(score_top_1 = 1.,\n",
    "                                    hpo_label = pl.col(\"hpo_label\").map_elements(lambda x: [x]),\n",
    "                                    snomed_label = pl.col(\"snomed_label\").map_elements(lambda x: [x]),\n",
    "                                    mapping_type = pl.lit(\"official\"))]\n",
    "                                    ,how= \"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "032e7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hop_snomed_mapping.write_parquet(os.path.join(Configs.output_mapping_folder, \"all_pheno_hpo_snomed_mapping.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b0956",
   "metadata": {},
   "source": [
    "# embed diseases and get mapping to SNOMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46953340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hop_snomed_mapping = pl.read_parquet(os.path.join(Configs.output_mapping_folder, \"all_pheno_hpo_snomed_mapping.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26334412",
   "metadata": {},
   "source": [
    "### embed disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed4549e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disease = df_hpoa[\"database_id\",'disease_name'].unique()\n",
    "df_disease.insert_column(0,pl.Series(np.arange(len(df_disease))).alias(\"index_disease\"))\n",
    "labels_disease = df_disease['disease_name'].to_list()\n",
    "index2disease = dict(zip(df_disease[\"index_disease\"], df_disease[\"database_id\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64c4780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cambridgeltl/SapBERT-from-PubMedBERT-fulltext. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc040e3195c4dd1b370a3781deff482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download from the 🤗 Hub\n",
    "base = Configs.embedding_model_hf\n",
    "# mine = \"yyzheng00/sapbert_lora_triplet_rank16_merged\"\n",
    "\n",
    "model = SentenceTransformer(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", device=Configs.device)\n",
    "embedding_disease = model.encode(labels_disease, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "637ae3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disease.write_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_disease.parquet\")\n",
    ")\n",
    "\n",
    "\n",
    "np.save(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_disease.npy\"),\n",
    "    embedding_disease\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2disease.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(index2disease, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61346bfd",
   "metadata": {},
   "source": [
    "### load diseases and snomed embedding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b5c0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_snomed = np.load(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_snomed.npy\")\n",
    ")\n",
    "embedding_disease = np.load(\n",
    "    os.path.join(Configs.output_embedding_folder, \"embedding_disease.npy\")\n",
    ")\n",
    "\n",
    "\n",
    "df_snomed_release_finding = pl.read_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_snomed_release_finding.parquet\")\n",
    ")\n",
    "df_disease = pl.read_parquet(\n",
    "    os.path.join(Configs.output_embedding_folder, \"df_disease.parquet\")\n",
    ")\n",
    "\n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2snomedid.pkl\"), \"rb\") as f:\n",
    "    index2snomedid = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(Configs.output_embedding_folder, \"index2disease.pkl\"), \"rb\") as f:\n",
    "    index2disease = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55344c07",
   "metadata": {},
   "source": [
    "### compute similarity disease concept mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d4eb120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/13 (0-1000)\n",
      "Processing batch 2/13 (1000-2000)\n",
      "Processing batch 3/13 (2000-3000)\n",
      "Processing batch 4/13 (3000-4000)\n",
      "Processing batch 5/13 (4000-5000)\n",
      "Processing batch 6/13 (5000-6000)\n",
      "Processing batch 7/13 (6000-7000)\n",
      "Processing batch 8/13 (7000-8000)\n",
      "Processing batch 9/13 (8000-9000)\n",
      "Processing batch 10/13 (9000-10000)\n",
      "Processing batch 11/13 (10000-11000)\n",
      "Processing batch 12/13 (11000-12000)\n",
      "Processing batch 13/13 (12000-12770)\n"
     ]
    }
   ],
   "source": [
    "all_top_scores, all_top_indices = top_k_array_by_batch(embedding_disease, embedding_snomed, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c94c5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_snomed_id = []\n",
    "disease_id_to_map = []\n",
    "mapped_snomed_label = []\n",
    "disease_label_to_map = []\n",
    "scores_mapping = []\n",
    "\n",
    "for disease_index in np.arange(len(all_top_scores)):\n",
    "    scores = all_top_scores[disease_index,:]\n",
    "    mapped_snomed_index = all_top_indices[disease_index,:]\n",
    "    score = scores[0]\n",
    "    \n",
    "    disease_ind = index2disease.get(disease_index)\n",
    "    snomed_ind = index2snomedid.get(mapped_snomed_index[0].item())\n",
    "\n",
    "    label_disease = df_disease.filter(pl.col(\"database_id\") == disease_ind)['disease_name']\n",
    "    label_snomed = df_snomed_release_finding.filter(pl.col(\"id\") == snomed_ind)['term']\n",
    "    \n",
    "    mapped_snomed_id.append(snomed_ind)\n",
    "    disease_id_to_map.append(disease_ind)\n",
    "    mapped_snomed_label.append(label_snomed)\n",
    "    disease_label_to_map.append(label_disease)\n",
    "    scores_mapping.append(score)\n",
    "\n",
    "df_snomed_disease_home_mapped = pl.DataFrame({\n",
    "    \"disease_id\": disease_id_to_map,\n",
    "    \"disease_label\": disease_label_to_map,\n",
    "    \"snomed_id\": mapped_snomed_id,\n",
    "    \"snomed_label\": mapped_snomed_label,\n",
    "    \"score_top_1\": scores_mapping,\n",
    "    \"mapping_type\": \"sapbert\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e934cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snomed_disease_home_mapped.write_parquet(os.path.join(Configs.output_mapping_folder, \"all_disease_hpo_snomed_mapping.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858751f0",
   "metadata": {},
   "source": [
    "# pheno - gene - disease association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46488ede",
   "metadata": {},
   "source": [
    "## load all mapping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88cb8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_mapping = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "baa65726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>disease_id</th><th>disease_label</th><th>snomed_id</th><th>snomed_label</th><th>score_top_1</th><th>mapping_type</th></tr><tr><td>str</td><td>list[str]</td><td>str</td><td>list[str]</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;OMIM:237800&quot;</td><td>[&quot;Hyperbilirubinemia, shunt, primary&quot;]</td><td>&quot;51071000&quot;</td><td>[&quot;Israel&#x27;s shunt hyperbilirubinemia&quot;, &quot;MAHA - Microangiopathic haemolytic anaemia&quot;, … &quot;Israel&#x27;s shunt hyperbilirubinaemia&quot;]</td><td>0.911101</td><td>&quot;sapbert&quot;</td></tr><tr><td>&quot;ORPHA:83617&quot;</td><td>[&quot;Agammaglobulinemia-microcephaly-craniosynostosis-severe dermatitis syndrome&quot;]</td><td>&quot;722281001&quot;</td><td>[&quot;Agammaglobulinaemia, microcephaly, craniosynostosis, severe dermatitis syndrome&quot;, &quot;Agammaglobulinemia, microcephaly, craniosynostosis, severe dermatitis syndrome (disorder)&quot;, &quot;Agammaglobulinemia, microcephaly, craniosynostosis, severe dermatitis syndrome&quot;]</td><td>0.989954</td><td>&quot;sapbert&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────────┬─────────────────────┬───────────┬─────────────────────┬─────────────┬──────────────┐\n",
       "│ disease_id  ┆ disease_label       ┆ snomed_id ┆ snomed_label        ┆ score_top_1 ┆ mapping_type │\n",
       "│ ---         ┆ ---                 ┆ ---       ┆ ---                 ┆ ---         ┆ ---          │\n",
       "│ str         ┆ list[str]           ┆ str       ┆ list[str]           ┆ f64         ┆ str          │\n",
       "╞═════════════╪═════════════════════╪═══════════╪═════════════════════╪═════════════╪══════════════╡\n",
       "│ OMIM:237800 ┆ [\"Hyperbilirubinemi ┆ 51071000  ┆ [\"Israel's shunt    ┆ 0.911101    ┆ sapbert      │\n",
       "│             ┆ a, shunt, p…        ┆           ┆ hyperbilirubi…      ┆             ┆              │\n",
       "│ ORPHA:83617 ┆ [\"Agammaglobulinemi ┆ 722281001 ┆ [\"Agammaglobulinaem ┆ 0.989954    ┆ sapbert      │\n",
       "│             ┆ a-microceph…        ┆           ┆ ia, microce…        ┆             ┆              │\n",
       "└─────────────┴─────────────────────┴───────────┴─────────────────────┴─────────────┴──────────────┘"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_mapping = pl.read_parquet(os.path.join(Configs.output_mapping_folder, \"all_disease_hpo_snomed_mapping.parquet\")).filter(pl.col(\"score_top_1\")>=threshold_mapping)\n",
    "pheno_mapping = pl.read_parquet(os.path.join(Configs.output_mapping_folder, \"all_pheno_hpo_snomed_mapping.parquet\")).filter(pl.col(\"score_top_1\")>=threshold_mapping)\n",
    "disease_mapping.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "60bb9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_p2g\n",
    " .join(pheno_mapping, on=\"hpo_id\")\n",
    " .select(\"snomed_id\", \"hpo_name\", \"gene_symbol\",\"disease_id\")\n",
    " .rename({\"snomed_id\" : \"pheno_snomed_id\"})\n",
    " .join(disease_mapping, on = \"disease_id\")\n",
    " .select(\"pheno_snomed_id\", \"hpo_name\", \"gene_symbol\",\"snomed_id\", \"disease_label\")\n",
    " .rename({\"snomed_id\" : \"disease_snomed_id\"})).unique().write_parquet(os.path.join(Configs.output_mapping_folder, \"pheno_to_gene_snomed_mapping.parquet\"))\n",
    "\n",
    "# df_g2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "215f402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_g2p\n",
    " .join(pheno_mapping, on=\"hpo_id\")\n",
    " .select(\"snomed_id\", \"hpo_name\", \"gene_symbol\",\"disease_id\")\n",
    " .rename({\"snomed_id\" : \"pheno_snomed_id\"})\n",
    " .join(disease_mapping, on = \"disease_id\")\n",
    " .select(\"pheno_snomed_id\", \"hpo_name\", \"gene_symbol\",\"snomed_id\", \"disease_label\")\n",
    " .rename({\"snomed_id\" : \"disease_snomed_id\"})).unique().write_parquet(os.path.join(Configs.output_mapping_folder, \"gene_to_pheno_snomed_mapping.parquet\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
